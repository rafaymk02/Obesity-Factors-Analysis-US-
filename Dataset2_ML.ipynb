{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fd7d0c5-735f-49de-887d-39ceb60b8f18",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f199b9e-7aad-4d70-adca-be7bb283ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial dataset shape: (93249, 33) (rows, columns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "df = pd.read_csv('physicaldataset2.csv')\n",
    "\n",
    "# Initial number of rows and columns\n",
    "print(f\"Initial dataset shape: {df.shape} (rows, columns)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e41606-9ba0-4774-80a6-0f2865e1cddd",
   "metadata": {},
   "source": [
    "# Displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ea6b42-f0a9-4448-9b37-1e6d0e5698bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YearStart                         0\n",
      "YearEnd                           0\n",
      "LocationAbbr                      0\n",
      "LocationDesc                      0\n",
      "Datasource                        0\n",
      "Class                             0\n",
      "Topic                             0\n",
      "Question                          0\n",
      "Data_Value_Unit               93249\n",
      "Data_Value_Type                   0\n",
      "Data_Value                     9235\n",
      "Data_Value_Alt                 9235\n",
      "Data_Value_Footnote_Symbol    84014\n",
      "Data_Value_Footnote           84014\n",
      "Low_Confidence_Limit           9235\n",
      "High_Confidence_Limit          9235\n",
      "Sample_Size                    9235\n",
      "Total                         89919\n",
      "Age(years)                    73269\n",
      "Education                     79929\n",
      "Gender                        86589\n",
      "Income                        69939\n",
      "Race/Ethnicity                66609\n",
      "GeoLocation                    1736\n",
      "ClassID                           0\n",
      "TopicID                           0\n",
      "QuestionID                        0\n",
      "DataValueTypeID                   0\n",
      "LocationID                        0\n",
      "StratificationCategory1           9\n",
      "Stratification1                   9\n",
      "StratificationCategoryId1         9\n",
      "StratificationID1                 9\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in each column\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe8eea-7627-42a4-aecc-79cfc331cc8b",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2ffdd8-672b-4fcf-b0ab-46a19f44d18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43274, 25)\n",
      "['YearStart', 'LocationAbbr', 'LocationDesc', 'Datasource', 'Class', 'Topic', 'Question', 'Data_Value_Type', 'Data_Value', 'Data_Value_Alt', 'Low_Confidence_Limit', 'High_Confidence_Limit ', 'Sample_Size', 'Total', 'Age(years)', 'Education', 'Gender', 'Income', 'Race/Ethnicity', 'GeoLocation', 'LocationID', 'StratificationCategory1', 'Stratification1', 'StratificationCategoryId1', 'StratificationID1']\n",
      "    YearStart LocationAbbr  LocationDesc  \\\n",
      "5        2015           GU          Guam   \n",
      "6        2012           WY       Wyoming   \n",
      "9        2011           AL       Alabama   \n",
      "11       2015           RI  Rhode Island   \n",
      "14       2020           DE      Delaware   \n",
      "\n",
      "                                    Datasource                    Class  \\\n",
      "5   Behavioral Risk Factor Surveillance System        Physical Activity   \n",
      "6   Behavioral Risk Factor Surveillance System  Obesity / Weight Status   \n",
      "9   Behavioral Risk Factor Surveillance System  Obesity / Weight Status   \n",
      "11  Behavioral Risk Factor Surveillance System  Obesity / Weight Status   \n",
      "14  Behavioral Risk Factor Surveillance System        Physical Activity   \n",
      "\n",
      "                           Topic  \\\n",
      "5   Physical Activity - Behavior   \n",
      "6        Obesity / Weight Status   \n",
      "9        Obesity / Weight Status   \n",
      "11       Obesity / Weight Status   \n",
      "14  Physical Activity - Behavior   \n",
      "\n",
      "                                             Question Data_Value_Type  \\\n",
      "5   Percent of adults who achieve at least 150 min...           Value   \n",
      "6   Percent of adults aged 18 years and older who ...           Value   \n",
      "9   Percent of adults aged 18 years and older who ...           Value   \n",
      "11  Percent of adults aged 18 years and older who ...           Value   \n",
      "14  Percent of adults who engage in no leisure-tim...           Value   \n",
      "\n",
      "    Data_Value  Data_Value_Alt  ...  Education        Gender        Income  \\\n",
      "5         27.4            27.4  ...        NaN  Missing Data  Missing Data   \n",
      "6         48.5            48.5  ...        NaN  Missing Data  Missing Data   \n",
      "9         35.2            35.2  ...        NaN  Missing Data  Missing Data   \n",
      "11        40.2            40.2  ...        NaN  Missing Data  Missing Data   \n",
      "14        15.3            15.3  ...        NaN  Missing Data  Missing Data   \n",
      "\n",
      "                   Race/Ethnicity                               GeoLocation  \\\n",
      "5                        Hispanic                   (13.444304, 144.793731)   \n",
      "6   American Indian/Alaska Native            (43.235541343, -108.109830353)   \n",
      "9                    Missing Data             (32.840571122, -86.631860762)   \n",
      "11                       Hispanic             (41.708280193, -71.522470314)   \n",
      "14                          Asian  (39.008830667000495, -75.57774116799965)   \n",
      "\n",
      "   LocationID StratificationCategory1                Stratification1  \\\n",
      "5          66          Race/Ethnicity                       Hispanic   \n",
      "6          56          Race/Ethnicity  American Indian/Alaska Native   \n",
      "9           1             Age (years)                        25 - 34   \n",
      "11         44          Race/Ethnicity                       Hispanic   \n",
      "14         10          Race/Ethnicity                          Asian   \n",
      "\n",
      "   StratificationCategoryId1 StratificationID1  \n",
      "5                       RACE           RACEHIS  \n",
      "6                       RACE           RACENAA  \n",
      "9                      AGEYR         AGEYR2534  \n",
      "11                      RACE           RACEHIS  \n",
      "14                      RACE           RACEASN  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "Final dataset shape: (43274, 25) (rows, columns)\n",
      "Total number of rows after cleaning: 43274\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "df = df.drop(['Data_Value_Unit', 'Data_Value_Footnote_Symbol', 'Data_Value_Footnote', 'DataValueTypeID', 'QuestionID', 'TopicID', 'ClassID'], axis=1)\n",
    "\n",
    "# Drop rows where 'GeoLocation' is missing\n",
    "df = df.dropna(subset=['GeoLocation'])\n",
    "\n",
    "# Remove rows with missing 'Data_Value'\n",
    "df = df.dropna(subset=['Data_Value'])\n",
    "\n",
    "# Remove rows where 'Age(years)', 'Gender', 'Race/Ethnicity', are all missing\n",
    "df = df.dropna(subset=['Age(years)', 'Gender', 'Race/Ethnicity'], how='all')\n",
    "\n",
    "# Fill in missing data for 'Age(years)', 'Gender', 'Race/Ethnicity', 'Income' with \"Missing Data\"\n",
    "for column in ['Age(years)', 'Gender', 'Race/Ethnicity', 'Income']:\n",
    "    df[column] = df[column].fillna('Missing Data')\n",
    "\n",
    "# Check if YearStart and YearEnd are always the same\n",
    "year_comparison = (df['YearStart'] == df['YearEnd']).all()\n",
    "if year_comparison:\n",
    "    # Drop YearEnd if it's always the same as YearStart\n",
    "    df = df.drop('YearEnd', axis=1)\n",
    "\n",
    "# Display the shape to see how many rows are left after all clean-up steps\n",
    "print(df.shape)\n",
    "\n",
    "# Check the remaining columns after cleanup\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "print(df.head())\n",
    "\n",
    "# Print the shape of the DataFrame after cleaning to show the number of rows and columns remaining\n",
    "print(f\"Final dataset shape: {df.shape} (rows, columns)\")\n",
    "\n",
    "print(f\"Total number of rows after cleaning: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f466e804-71b5-4522-bbc0-881f18cbb003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Age(years)': ['Missing Data' '25 - 34' '55 - 64' '45 - 54' '35 - 44' '18 - 24'\n",
      " '65 or older']\n",
      "Unique values in 'Gender': ['Missing Data' 'Male' 'Female']\n",
      "Unique values in 'Race/Ethnicity': ['Hispanic' 'American Indian/Alaska Native' 'Missing Data' 'Asian' 'Other'\n",
      " '2 or more races' 'Non-Hispanic White' 'Hawaiian/Pacific Islander'\n",
      " 'Non-Hispanic Black']\n",
      "Unique values in 'Income': ['Missing Data']\n",
      "Unique locations: ['Guam' 'Wyoming' 'Alabama' 'Rhode Island' 'Delaware' 'New Jersey'\n",
      " 'Puerto Rico' 'Maine' 'Virginia' 'Washington' 'California' 'New York'\n",
      " 'Massachusetts' 'Arkansas' 'Illinois' 'New Hampshire' 'Maryland' 'Hawaii'\n",
      " 'Louisiana' 'South Dakota' 'Texas' 'Oklahoma' 'Oregon' 'Kansas' 'Florida'\n",
      " 'Idaho' 'Virgin Islands' 'Montana' 'District of Columbia' 'Minnesota'\n",
      " 'Colorado' 'North Carolina' 'North Dakota' 'South Carolina'\n",
      " 'Pennsylvania' 'Nebraska' 'Michigan' 'Nevada' 'New Mexico' 'Wisconsin'\n",
      " 'Utah' 'Arizona' 'Mississippi' 'Indiana' 'Georgia' 'Ohio' 'Iowa'\n",
      " 'Kentucky' 'Missouri' 'Connecticut' 'Alaska' 'Vermont' 'West Virginia'\n",
      " 'Tennessee']\n"
     ]
    }
   ],
   "source": [
    "# Display unique values for key columns\n",
    "print(\"Unique values in 'Age(years)':\", df['Age(years)'].unique())\n",
    "print(\"Unique values in 'Gender':\", df['Gender'].unique())\n",
    "print(\"Unique values in 'Race/Ethnicity':\", df['Race/Ethnicity'].unique())\n",
    "print(\"Unique values in 'Income':\", df['Income'].unique())\n",
    "unique_locations = df['LocationDesc'].unique()\n",
    "print(\"Unique locations:\", unique_locations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b9448-38fa-41b5-be46-d99b97e308fd",
   "metadata": {},
   "source": [
    "# Gathering Data for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa162cd-c973-48ea-adf1-0041b756b3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation Matrix:\n",
      "                        YearStart  Data_Value  Data_Value_Alt  \\\n",
      "YearStart                1.000000   -0.000904       -0.000904   \n",
      "Data_Value              -0.000904    1.000000        1.000000   \n",
      "Data_Value_Alt          -0.000904    1.000000        1.000000   \n",
      "Low_Confidence_Limit    -0.001535    0.936790        0.936790   \n",
      "High_Confidence_Limit   -0.001632    0.916696        0.916696   \n",
      "Sample_Size             -0.053299   -0.000841       -0.000841   \n",
      "LocationID               0.037441    0.006923        0.006923   \n",
      "\n",
      "                        Low_Confidence_Limit  High_Confidence_Limit   \\\n",
      "YearStart                          -0.001535               -0.001632   \n",
      "Data_Value                          0.936790                0.916696   \n",
      "Data_Value_Alt                      0.936790                0.916696   \n",
      "Low_Confidence_Limit                1.000000                0.722091   \n",
      "High_Confidence_Limit               0.722091                1.000000   \n",
      "Sample_Size                         0.194919               -0.216505   \n",
      "LocationID                         -0.000150                0.014075   \n",
      "\n",
      "                        Sample_Size  LocationID  \n",
      "YearStart                 -0.053299    0.037441  \n",
      "Data_Value                -0.000841    0.006923  \n",
      "Data_Value_Alt            -0.000841    0.006923  \n",
      "Low_Confidence_Limit       0.194919   -0.000150  \n",
      "High_Confidence_Limit     -0.216505    0.014075  \n",
      "Sample_Size                1.000000   -0.013561  \n",
      "LocationID                -0.013561    1.000000  \n",
      "\n",
      "P-values:\n",
      "YearStart: 0.00000\n",
      "Data_Value: 0.00000\n",
      "Data_Value_Alt: 0.00000\n",
      "Low_Confidence_Limit: 0.00000\n",
      "High_Confidence_Limit : 0.00000\n",
      "Sample_Size: 0.00007\n",
      "LocationID: 0.02522\n",
      "\n",
      "Feature Importances:\n",
      "YearStart: 0.14381\n",
      "Data_Value: 0.13606\n",
      "Data_Value_Alt: 0.13338\n",
      "Low_Confidence_Limit: 0.15041\n",
      "High_Confidence_Limit : 0.14649\n",
      "Sample_Size: 0.16080\n",
      "LocationID: 0.12904\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Class', axis=1)  # Features\n",
    "y = df['Class']  # Target variable\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Remove non-numeric columns from the feature set\n",
    "X_numeric = X.drop(columns=non_numeric_columns)\n",
    "\n",
    "# Correlation analysis\n",
    "correlation_matrix = X_numeric.corr()\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Statistical tests (ANOVA F-test)\n",
    "selector = SelectKBest(score_func=f_classif, k=5)  # Select top 5 features\n",
    "selector.fit(X_numeric, y)\n",
    "p_values = selector.pvalues_\n",
    "print(\"\\nP-values:\")\n",
    "for feature, p_value in zip(X_numeric.columns, p_values):\n",
    "    print(f\"{feature}: {p_value:.5f}\")\n",
    "\n",
    "# Feature importance scores from Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_numeric, y)\n",
    "importances = rf_model.feature_importances_\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(X_numeric.columns, importances):\n",
    "    print(f\"{feature}: {importance:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfcccf8-5789-4e50-a811-62b01500a24f",
   "metadata": {},
   "source": [
    "# Testing the ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a37dae7b-a940-4bc0-a2da-e879834594e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.5377238590410167\n",
      "Precision: 0.47530423499374425\n",
      "Recall: 0.5377238590410167\n",
      "F1-score: 0.48543787340070627\n",
      "\n",
      "Random Forest:\n",
      "Accuracy: 0.6614673599075679\n",
      "Precision: 0.6519165547082781\n",
      "Recall: 0.6614673599075679\n",
      "F1-score: 0.6539979578807535\n"
     ]
    }
   ],
   "source": [
    "X = df.drop('Class', axis=1)  # Features\n",
    "y = df['Class']  # Target variable\n",
    "\n",
    "# Identify non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Remove non-numeric columns from the feature set\n",
    "X_numeric = X.drop(columns=non_numeric_columns)\n",
    "\n",
    "selected_features = ['YearStart', 'Data_Value', 'Low_Confidence_Limit', 'High_Confidence_Limit ', 'Sample_Size', 'LocationID']\n",
    "X_selected = X_numeric[selected_features]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Train a random forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "lr_predictions = lr_model.predict(X_test_scaled)\n",
    "rf_predictions = rf_model.predict(X_test_scaled)\n",
    "\n",
    "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "\n",
    "lr_precision = precision_score(y_test, lr_predictions, average='weighted')\n",
    "rf_precision = precision_score(y_test, rf_predictions, average='weighted')\n",
    "\n",
    "lr_recall = recall_score(y_test, lr_predictions, average='weighted')\n",
    "rf_recall = recall_score(y_test, rf_predictions, average='weighted')\n",
    "\n",
    "lr_f1 = f1_score(y_test, lr_predictions, average='weighted')\n",
    "rf_f1 = f1_score(y_test, rf_predictions, average='weighted')\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", lr_accuracy)\n",
    "print(\"Precision:\", lr_precision)\n",
    "print(\"Recall:\", lr_recall)\n",
    "print(\"F1-score:\", lr_f1)\n",
    "\n",
    "print(\"\\nRandom Forest:\")\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print(\"Precision:\", rf_precision)\n",
    "print(\"Recall:\", rf_recall)\n",
    "print(\"F1-score:\", rf_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
